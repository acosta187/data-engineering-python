# ğŸ“Š Sistema de PredicciÃ³n de Riesgo CardÃ­aco
## Pipeline de Datos MÃ©dicos con Arquitectura Medallion

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![XGBoost](https://img.shields.io/badge/XGBoost-2.0+-green.svg)](https://xgboost.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‘ Tabla de Contenidos

- [Arquitectura del Sistema](#-arquitectura-del-sistema)
- [Estructura de Directorios](#-estructura-de-directorios)
- [Flujo de Datos](#-flujo-de-datos-data-pipeline)
- [DescripciÃ³n de Componentes](#-descripciÃ³n-de-componentes)
- [Zonas de Datos](#-zonas-de-datos)
- [GuÃ­a de InstalaciÃ³n](#-guÃ­a-de-instalaciÃ³n)
- [GuÃ­a de Uso](#-guÃ­a-de-uso)
- [AnÃ¡lisis de Datos](#-anÃ¡lisis-de-datos)
- [Optimizaciones Avanzadas](#-optimizaciones-avanzadas)
- [Consideraciones de ProducciÃ³n](#-consideraciones-de-producciÃ³n)
- [Troubleshooting](#-troubleshooting)
- [Referencias TÃ©cnicas](#-referencias-tÃ©cnicas)

---

## ğŸ—ï¸ Arquitectura del Sistema

Este proyecto implementa un **patrÃ³n medallion simplificado** (Bronze â†’ Gold) para el procesamiento de datos mÃ©dicos, combinando:

- **Machine Learning:** PredicciÃ³n de riesgo cardÃ­aco con XGBoost
- **Data Engineering:** Pipeline ETL incremental con Avro y Parquet
- **Web Application:** Interfaz de usuario con Streamlit

### Principios de DiseÃ±o

1. **SeparaciÃ³n de Responsabilidades (SoC):** Cada componente tiene una Ãºnica responsabilidad bien definida
2. **Inmutabilidad:** Los datos en la zona Archive nunca se modifican
3. **Idempotencia:** Ejecutar el pipeline mÃºltiples veces produce el mismo resultado
4. **AuditorÃ­a Completa:** Trazabilidad de cada registro desde su origen

### Diagrama de Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPA DE PRESENTACIÃ“N                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         streamlit_app_arvo.py (Interfaz Web)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPA DE MODELADO ML                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   code_ml.py    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  ml_model/                   â”‚   â”‚
â”‚  â”‚  (Entrenamiento)â”‚         â”‚  â”œâ”€â”€ model_xgb.pkl           â”‚   â”‚
â”‚  â”‚                 â”‚         â”‚  â””â”€â”€ transformador.pkl       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CAPA DE INGENIERÃA DE DATOS (ETL)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              arvo_parket.py (Pipeline ETL)               â”‚   â”‚
â”‚  â”‚  Extract â†’ Transform â†’ Load â†’ Archive                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CAPA DE ALMACENAMIENTO                         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  BRONZE (Landing)   â”‚  â”‚  ARCHIVE (HistÃ³rico)             â”‚ â”‚
â”‚  â”‚  registros_         â”‚  â”‚  registros_procesados/           â”‚ â”‚
â”‚  â”‚  pacientes/         â”‚  â”‚  â”œâ”€â”€ record_..._Manuel.avro      â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ nuevo1.avro    â”‚  â”‚  â”œâ”€â”€ record_..._Ricardo.avro    â”‚ â”‚
â”‚  â”‚  â””â”€â”€ nuevo2.avro    â”‚  â”‚  â””â”€â”€ record_..._Ita.avro        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  GOLD (Data Lake Optimizado)                             â”‚  â”‚
â”‚  â”‚  datalake_maestro.parquet                                â”‚  â”‚
â”‚  â”‚  (Formato Columnar - Consultas RÃ¡pidas)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Estructura de Directorios

```
arvo_parquet/
â”‚
â”œâ”€â”€ ğŸ–¥ï¸ CAPA DE PRESENTACIÃ“N
â”‚   â””â”€â”€ streamlit_app_arvo.py          # Interfaz web para captura de datos
â”‚
â”œâ”€â”€ ğŸ§  CAPA DE MODELADO ML
â”‚   â”œâ”€â”€ code_ml.py                     # Entrenamiento y lÃ³gica del modelo
â”‚   â”œâ”€â”€ heart_disease_uci.csv          # Dataset original (UCI Heart Disease)
â”‚   â”‚
â”‚   â””â”€â”€ ml_model/                      # ğŸ’¾ Artefactos del modelo
â”‚       â”œâ”€â”€ model_xgb.pkl              # Modelo XGBoost entrenado (2.3 MB)
â”‚       â””â”€â”€ transformador.pkl          # Pipeline de preprocesamiento (45 KB)
â”‚
â”œâ”€â”€ âš™ï¸ CAPA DE INGENIERÃA DE DATOS
â”‚   â””â”€â”€ arvo_parket.py                 # Script ETL (Avro â†’ Parquet)
â”‚
â”œâ”€â”€ ğŸ“¥ ZONA BRONZE (Landing Zone)
â”‚   â””â”€â”€ registros_pacientes/           # Archivos .avro en crudo
â”‚       â””â”€â”€ [Archivos pendientes de procesar]
â”‚
â”œâ”€â”€ ğŸ“¦ ZONA ARCHIVE (HistÃ³rico)
â”‚   â””â”€â”€ registros_procesados/          # .avro procesados (auditorÃ­a)
â”‚       â”œâ”€â”€ record_20260101_212921_Manuel.avro
â”‚       â”œâ”€â”€ record_20260101_213010_Ricardo.avro
â”‚       â””â”€â”€ record_20260101_213032_Ita.avro
â”‚
â””â”€â”€ ğŸ† ZONA GOLD (Data Lake Optimizado)
    â””â”€â”€ datalake_maestro.parquet       # Base de datos columnar unificada
```

### Detalles de Archivos Clave

| Archivo | TamaÃ±o Aprox. | PropÃ³sito |
|---------|---------------|-----------|
| `streamlit_app_arvo.py` | ~15 KB | Interfaz de usuario y lÃ³gica de predicciÃ³n |
| `code_ml.py` | ~8 KB | Entrenamiento del modelo XGBoost |
| `arvo_parket.py` | ~5 KB | Pipeline ETL automatizable |
| `model_xgb.pkl` | ~2.3 MB | Modelo entrenado serializado |
| `transformador.pkl` | ~45 KB | Preprocesador (scaler + encoder) |
| `heart_disease_uci.csv` | ~8 KB | Dataset de 303 pacientes |
| `datalake_maestro.parquet` | Variable | Base de datos columnar (crece con el tiempo) |

---

## ğŸ”„ Flujo de Datos (Data Pipeline)

### DescripciÃ³n del Flujo Paso a Paso

#### Fase 1: Captura y PredicciÃ³n (Tiempo Real)
1. **Usuario ingresa datos** en formulario Streamlit
2. **Modelo ML procesa** los datos usando transformador + XGBoost
3. **Resultado mostrado** en pantalla (probabilidad de riesgo)
4. **Persistencia atÃ³mica** del registro en formato Avro

#### Fase 2: ConsolidaciÃ³n (Batch)
5. **Trigger manual/automÃ¡tico** ejecuta `arvo_parket.py`
6. **ExtracciÃ³n** de todos los `.avro` pendientes
7. **TransformaciÃ³n** a DataFrame unificado con validaciones
8. **Carga** incremental a Parquet (append o merge)
9. **Archivo** de registros procesados para auditorÃ­a

#### Fase 3: Consumo (AnÃ¡lisis)
10. **Herramientas BI** conectan directamente al Parquet
11. **Scripts Python** realizan anÃ¡lisis estadÃ­sticos

---

## ğŸ§© DescripciÃ³n de Componentes

### 1ï¸âƒ£ **streamlit_app_arvo.py** - Interfaz de Usuario

**Responsabilidades:**
- âœ… Capturar datos demogrÃ¡ficos y clÃ­nicos del paciente
- âœ… Validar entrada del usuario (rangos, tipos de datos)
- âœ… Invocar modelo ML para predicciÃ³n en tiempo real
- âœ… Persistir cada registro como archivo Avro individual
- âœ… Asignar nombres de archivo con timestamp Ãºnico

**TecnologÃ­as:**
- `streamlit` - Framework web interactivo
- `fastavro` - SerializaciÃ³n Avro
- `joblib` - Carga de modelos pickle
- `pandas` - ManipulaciÃ³n de datos

**Ejemplo de CÃ³digo (Simplificado):**

```python
import streamlit as st
import joblib
import fastavro
from datetime import datetime

# Cargar modelo
modelo = joblib.load('ml_model/model_xgb.pkl')
transformador = joblib.load('ml_model/transformador.pkl')

# Formulario
st.title("ğŸ¥ PredicciÃ³n de Riesgo CardÃ­aco")
nombre = st.text_input("Nombre del paciente")
edad = st.number_input("Edad", min_value=18, max_value=120)
sexo = st.selectbox("Sexo", ["Masculino", "Femenino"])
presion = st.number_input("PresiÃ³n arterial sistÃ³lica (mmHg)")

if st.button("Predecir Riesgo"):
    # Preparar datos
    datos = {
        'nombre': nombre,
        'edad': edad,
        'sexo': sexo,
        'presion_arterial': presion,
    }
    
    # PredicciÃ³n
    X = transformador.transform([datos])
    probabilidad = modelo.predict_proba(X)[0][1]
    
    st.success(f"Probabilidad de riesgo: {probabilidad:.2%}")
    
    # Guardar en Avro
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"registros_pacientes/record_{timestamp}_{nombre}.avro"
    
    esquema = {
        "type": "record",
        "name": "RegistroPaciente",
        "fields": [
            {"name": "nombre", "type": "string"},
            {"name": "edad", "type": "int"},
            {"name": "probabilidad_riesgo", "type": "double"},
        ]
    }
    
    with open(filename, 'wb') as f:
        fastavro.writer(f, esquema, [datos])
    
    st.info(f"âœ… Registro guardado: {filename}")
```

**CaracterÃ­sticas Clave:**
- **PredicciÃ³n InstantÃ¡nea:** Sin latencia perceptible (< 100ms)
- **ValidaciÃ³n en Tiempo Real:** Feedback inmediato al usuario
- **Persistencia AtÃ³mica:** Cada escritura es una transacciÃ³n completa
- **Naming Convention:** `record_YYYYMMDD_HHMMSS_Nombre.avro`

---

### 2ï¸âƒ£ **code_ml.py** - Motor de Machine Learning

**Responsabilidades:**
- âœ… Cargar y limpiar dataset UCI Heart Disease
- âœ… IngenierÃ­a de caracterÃ­sticas (feature engineering)
- âœ… Entrenar modelo XGBoost con validaciÃ³n cruzada
- âœ… Optimizar hiperparÃ¡metros (opcional: GridSearch)
- âœ… Serializar modelo y transformador

**Algoritmo:** XGBoost Classifier (Gradient Boosting)

**Pipeline de Preprocesamiento:**

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline

# Definir columnas numÃ©ricas y categÃ³ricas
cols_numericas = ['edad', 'presion_arterial', 'colesterol', 'freq_cardiaca_max']
cols_categoricas = ['sexo', 'tipo_dolor_pecho', 'azucar_sangre', 'ecg_reposo']

# Crear transformador
transformador = ColumnTransformer([
    ('scaler_num', StandardScaler(), cols_numericas),
    ('encoder_cat', OneHotEncoder(drop='first', sparse_output=False), cols_categoricas)
])

# Pipeline completo
pipeline = Pipeline([
    ('transformador', transformador),
    ('modelo', XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    ))
])
```

**Flujo de Entrenamiento:**

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
import joblib

# 1. Cargar datos
df = pd.read_csv('heart_disease_uci.csv')

# 2. Limpieza
df = df.dropna()

# 3. Split train/test
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. Entrenar
pipeline.fit(X_train, y_train)

# 5. Evaluar
from sklearn.metrics import roc_auc_score
y_pred_proba = pipeline.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_pred_proba)
print(f"AUC-ROC: {auc:.3f}")

# 6. Guardar
joblib.dump(pipeline.named_steps['modelo'], 'ml_model/model_xgb.pkl')
joblib.dump(pipeline.named_steps['transformador'], 'ml_model/transformador.pkl')
```

**MÃ©tricas TÃ­picas:**
- **AUC-ROC:** 0.85 - 0.90 (excelente discriminaciÃ³n)
- **Accuracy:** ~85%
- **Sensibilidad (Recall):** ~82% (detecta 82% de casos positivos)
- **Especificidad:** ~87% (descarta 87% de casos negativos)

---

### 3ï¸âƒ£ **arvo_parket.py** - Pipeline ETL

**Responsabilidades:**
- âœ… **Extract:** Leer todos los `.avro` de `registros_pacientes/`
- âœ… **Transform:** Unificar en DataFrame Pandas con validaciÃ³n de esquema
- âœ… **Load:** Escribir/anexar datos a `datalake_maestro.parquet`
- âœ… **Archive:** Mover archivos procesados a zona histÃ³rica
- âœ… **Logging:** Registrar operaciones y errores

**CÃ³digo Completo:**

```python
import os
import glob
import shutil
import pandas as pd
import fastavro
import pyarrow.parquet as pq
from datetime import datetime

def procesar_pipeline():
    """
    Pipeline ETL completo para consolidar archivos Avro a Parquet
    """
    
    # ===== EXTRACT =====
    print("ğŸ” Fase 1: Extrayendo archivos Avro...")
    archivos_pendientes = glob.glob('registros_pacientes/*.avro')
    
    if not archivos_pendientes:
        print("âš ï¸  No hay archivos pendientes de procesar")
        return
    
    print(f"   Encontrados: {len(archivos_pendientes)} archivos")
    
    # ===== TRANSFORM =====
    print("\nğŸ”„ Fase 2: Transformando datos...")
    registros = []
    
    for archivo in archivos_pendientes:
        try:
            with open(archivo, 'rb') as f:
                avro_reader = fastavro.reader(f)
                for registro in avro_reader:
                    # Agregar metadatos de auditorÃ­a
                    registro['_ingested_at'] = datetime.now().isoformat()
                    registro['_source_file'] = os.path.basename(archivo)
                    registros.append(registro)
        except Exception as e:
            print(f"   âŒ Error leyendo {archivo}: {e}")
            continue
    
    if not registros:
        print("   âš ï¸  No se pudieron extraer registros vÃ¡lidos")
        return
    
    df_nuevo = pd.DataFrame(registros)
    print(f"   âœ… Unificados {len(df_nuevo)} registros")
    
    # ValidaciÃ³n de esquema
    columnas_requeridas = ['nombre', 'edad', 'probabilidad_riesgo']
    if not all(col in df_nuevo.columns for col in columnas_requeridas):
        raise ValueError("Esquema invÃ¡lido: faltan columnas requeridas")
    
    # ===== LOAD =====
    print("\nğŸ’¾ Fase 3: Cargando a Parquet...")
    parquet_path = 'datalake_maestro.parquet'
    
    if os.path.exists(parquet_path):
        # Append incremental
        df_existente = pd.read_parquet(parquet_path)
        df_consolidado = pd.concat([df_existente, df_nuevo], ignore_index=True)
        
        # DeduplicaciÃ³n
        df_consolidado = df_consolidado.drop_duplicates(
            subset=['nombre', '_ingested_at'], 
            keep='last'
        )
    else:
        # Primera escritura
        df_consolidado = df_nuevo
    
    # Escribir con compresiÃ³n
    df_consolidado.to_parquet(
        parquet_path,
        engine='pyarrow',
        compression='snappy',
        index=False
    )
    
    print(f"   âœ… Escritos {len(df_consolidado)} registros totales")
    print(f"   ğŸ“¦ TamaÃ±o: {os.path.getsize(parquet_path) / 1024:.2f} KB")
    
    # ===== ARCHIVE =====
    print("\nğŸ“¦ Fase 4: Archivando procesados...")
    os.makedirs('registros_procesados', exist_ok=True)
    
    for archivo in archivos_pendientes:
        destino = os.path.join('registros_procesados', os.path.basename(archivo))
        shutil.move(archivo, destino)
        print(f"   âœ… Movido: {os.path.basename(archivo)}")
    
    print("\nğŸ‰ Pipeline completado exitosamente")

if __name__ == "__main__":
    procesar_pipeline()
```

---

## ğŸ—„ï¸ Zonas de Datos

### Arquitectura Medallion Simplificada

| Zona | Directorio | Formato | PropÃ³sito | CaracterÃ­sticas |
|------|-----------|---------|-----------|-----------------|
| **Bronze** | `registros_pacientes/` | Avro | Ingesta transaccional rÃ¡pida | Orientado a fila, esquema embebido |
| **Archive** | `registros_procesados/` | Avro | AuditorÃ­a e historial inmutable | Backup completo, no se modifica |
| **Gold** | `datalake_maestro.parquet` | Parquet | Consultas analÃ­ticas optimizadas | Orientado a columna, compresiÃ³n inteligente |

### Â¿Por quÃ© Avro en Bronze?

**Ventajas:**
- âœ… **Orientado a fila:** Ideal para escrituras de un registro a la vez
- âœ… **Esquema embebido:** AutovalidaciÃ³n de estructura
- âœ… **Compacto:** SerializaciÃ³n binaria eficiente (50% mÃ¡s pequeÃ±o que JSON)
- âœ… **EvoluciÃ³n de esquema:** Compatible con cambios hacia adelante/atrÃ¡s

**Casos de uso ideales:**
- Logs de eventos en tiempo real
- Sistemas transaccionales (OLTP)
- Streaming de datos (Kafka)

### Â¿Por quÃ© Parquet en Gold?

**Ventajas:**
- âœ… **Orientado a columna:** Consultas 100x mÃ¡s rÃ¡pidas para agregaciones
- âœ… **CompresiÃ³n inteligente:** 5-10x menos espacio que CSV
- âœ… **Predicate pushdown:** Lee solo las columnas y filas necesarias
- âœ… **Compatible con BI:** Power BI, Tableau, Apache Spark, DuckDB

**Casos de uso ideales:**
- AnÃ¡lisis de datos (OLAP)
- Data warehouses
- Machine Learning feature stores

---

## ğŸš€ GuÃ­a de InstalaciÃ³n

### Prerequisitos

- **Python:** 3.9 o superior
- **Sistema Operativo:** Linux, macOS, o Windows
- **RAM:** MÃ­nimo 2 GB (recomendado 4 GB)
- **Disco:** 500 MB libres

### Paso 1: Crear Entorno Virtual

```bash
# Linux/Mac
python3 -m venv webdjango
source webdjango/bin/activate

# Windows
python -m venv webdjango
webdjango\Scripts\activate
```

### Paso 2: Instalar Dependencias

```bash
# Actualizar pip
pip install --upgrade pip

# Instalar paquetes core
pip install streamlit pandas numpy scikit-learn xgboost joblib fastavro pyarrow

# Verificar instalaciÃ³n
python -c "import streamlit; print(f'Streamlit {streamlit.__version__}')"
python -c "import xgboost; print(f'XGBoost {xgboost.__version__}')"
```

### Paso 3: Crear Estructura de Directorios

```bash
mkdir -p registros_pacientes registros_procesados ml_model
```

### VerificaciÃ³n de InstalaciÃ³n

```bash
# Debe mostrar las versiones instaladas
pip list | grep -E "streamlit|xgboost|fastavro|pyarrow"
```

**Output esperado:**
```
fastavro           1.9.0
pyarrow            14.0.1
streamlit          1.28.2
xgboost            2.0.3
```

---

## ğŸ“– GuÃ­a de Uso

### Flujo Completo de Trabajo

#### Paso 1: Entrenar el Modelo (Una sola vez)

```bash
python code_ml.py
```

**Output esperado:**

```
ğŸ§  Iniciando entrenamiento del modelo...

ğŸ“Š Cargando dataset UCI Heart Disease...
   âœ… Cargados 303 registros

ğŸ”„ Preprocesando datos...
   âœ… ImputaciÃ³n de valores faltantes completada
   âœ… Split train/test: 242 / 61 registros

ğŸš€ Entrenando XGBoost Classifier...
   [0]     validation_0-logloss:0.62341
   [10]    validation_0-logloss:0.45123
   [20]    validation_0-logloss:0.38456
   [50]    validation_0-logloss:0.32109
   [99]    validation_0-logloss:0.29871

âœ… Entrenamiento completado

ğŸ“Š MÃ©tricas en conjunto de prueba:
   - AUC-ROC: 0.873
   - Accuracy: 0.852
   - Sensibilidad: 0.821
   - Especificidad: 0.876

ğŸ’¾ Guardando artefactos...
   âœ… ml_model/model_xgb.pkl (2.3 MB)
   âœ… ml_model/transformador.pkl (45 KB)

ğŸ‰ Modelo entrenado exitosamente
```

---

#### Paso 2: Capturar Datos de Pacientes

```bash
streamlit run streamlit_app_arvo.py
```

**Acciones en la interfaz:**

1. **Abrir navegador** en `http://localhost:8501`
2. **Completar formulario mÃ©dico:**
   - Nombre del paciente
   - Edad (18-120 aÃ±os)
   - Sexo (Masculino/Femenino)
   - PresiÃ³n arterial sistÃ³lica (mmHg)
   - Colesterol sÃ©rico (mg/dl)
   - Frecuencia cardÃ­aca mÃ¡xima
   - Y mÃ¡s campos clÃ­nicos...

3. **Click en "Predecir Riesgo"**
4. **Ver resultado:**
   ```
   ğŸ¯ Probabilidad de Riesgo CardÃ­aco: 68.3%
   âš ï¸  ClasificaciÃ³n: Riesgo Alto
   ```

5. **ConfirmaciÃ³n de guardado:**
   ```
   âœ… Registro guardado exitosamente
   ğŸ“„ Archivo: registros_pacientes/record_20260101_152030_JuanPerez.avro
   ```

**Resultado:** Se crea automÃ¡ticamente un archivo `.avro` en la carpeta `registros_pacientes/`

---

#### Paso 3: Consolidar a Parquet (ETL)

```bash
python arvo_parket.py
```

**Output en consola:**

```
ğŸ” Fase 1: Extrayendo archivos Avro...
   Encontrados: 5 archivos

ğŸ”„ Fase 2: Transformando datos...
   âœ… Unificados 5 registros
   âœ… ValidaciÃ³n de esquema: OK

ğŸ’¾ Fase 3: Cargando a Parquet...
   â„¹ï¸  Detectado archivo existente, anexando...
   âœ… Escritos 23 registros totales
   ï¿½ï¿½ TamaÃ±o del archivo: 3.47 KB
   ğŸ“ˆ Tasa de compresiÃ³n: 8.2x vs CSV

ğŸ“¦ Fase 4: Archivando procesados...
   âœ… Movido: record_20260101_212921_Manuel.avro
   âœ… Movido: record_20260101_213010_Ricardo.avro
   âœ… Movido: record_20260101_213032_Ita.avro
   âœ… Movido: record_20260101_214505_Ana.avro
   âœ… Movido: record_20260101_215120_Carlos.avro

ğŸ‰ Pipeline completado exitosamente
   â±ï¸  Tiempo de ejecuciÃ³n: 0.23 segundos
```

---

#### Paso 4: Consultar Data Lake (AnÃ¡lisis)

```python
import pandas as pd
import pyarrow.parquet as pq

# ===== CONSULTA BÃSICA =====
# Leer solo columnas necesarias (optimizaciÃ³n)
df = pd.read_parquet(
    'datalake_maestro.parquet',
    columns=['nombre', 'edad', 'probabilidad_riesgo']
)

print(df.head())
```

**Output:**
```
           nombre  edad  probabilidad_riesgo
0          Manuel    45                0.683
1         Ricardo    62                0.891
2             Ita    38                0.234
3             Ana    55                0.756
4          Carlos    71                0.923
```

```python
# ===== ANÃLISIS ESTADÃSTICO =====
print(f"Total de pacientes: {len(df)}")
print(f"Riesgo promedio: {df['probabilidad_riesgo'].mean():.2%}")
print(f"Edad promedio: {df['edad'].mean():.1f} aÃ±os")

# Pacientes de alto riesgo (>70%)
alto_riesgo = df[df['probabilidad_riesgo'] > 0.7]
print(f"Pacientes de alto riesgo: {len(alto_riesgo)} ({len(alto_riesgo)/len(df):.1%})")

# ===== CONSULTA CON FILTROS =====
# Gracias a Parquet, esto es ultra rÃ¡pido (predicate pushdown)
pacientes_mayores = pd.read_parquet(
    'datalake_maestro.parquet',
    filters=[('edad', '>', 60)]
)

# ===== EXPORTAR A OTROS FORMATOS =====
df.to_csv('reporte_pacientes.csv', index=False)
df.to_excel('reporte_pacientes.xlsx', index=False)

print("âœ… Reportes exportados")
```

---

## ğŸ“Š AnÃ¡lisis de Datos

### Ejemplo 1: Dashboard Simple con Pandas

```python
import pandas as pd
import matplotlib.pyplot as plt

# Cargar datos
df = pd.read_parquet('datalake_maestro.parquet')

# DistribuciÃ³n de riesgo por edad
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# GrÃ¡fico 1: Histograma de edad
axes[0].hist(df['edad'], bins=20, edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Edad')
axes[0].set_ylabel('Frecuencia')
axes[0].set_title('DistribuciÃ³n de Edad')

# GrÃ¡fico 2: Boxplot de riesgo por sexo
df.boxplot(column='probabilidad_riesgo', by='sexo', ax=axes[1])
axes[1].set_xlabel('Sex# ğŸ“Š Sistema de PredicciÃ³n de Riesgo CardÃ­aco
## Pipeline de Datos MÃ©dicos con Arquitectura Medallion

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![XGBoost](https://img.shields.io/badge/XGBoost-2.0+-green.svg)](https://xgboost.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‘ Tabla de Contenidos

- [Arquitectura del Sistema](#-arquitectura-del-sistema)
- [Estructura de Directorios](#-estructura-de-directorios)
- [Flujo de Datos](#-flujo-de-datos-data-pipeline)
- [DescripciÃ³n de Componentes](#-descripciÃ³n-de-componentes)
- [Zonas de Datos](#-zonas-de-datos)
- [GuÃ­a de InstalaciÃ³n](#-guÃ­a-de-instalaciÃ³n)
- [GuÃ­a de Uso](#-guÃ­a-de-uso)
- [AnÃ¡lisis de Datos](#-anÃ¡lisis-de-datos)
- [Optimizaciones Avanzadas](#-optimizaciones-avanzadas)
- [Consideraciones de ProducciÃ³n](#-consideraciones-de-producciÃ³n)
- [Troubleshooting](#-troubleshooting)
- [Referencias TÃ©cnicas](#-referencias-tÃ©cnicas)

---

## ğŸ—ï¸ Arquitectura del Sistema

Este proyecto implementa un **patrÃ³n medallion simplificado** (Bronze â†’ Gold) para el procesamiento de datos mÃ©dicos, combinando:

- **Machine Learning:** PredicciÃ³n de riesgo cardÃ­aco con XGBoost
- **Data Engineering:** Pipeline ETL incremental con Avro y Parquet
- **Web Application:** Interfaz de usuario con Streamlit

### Principios de DiseÃ±o

1. **SeparaciÃ³n de Responsabilidades (SoC):** Cada componente tiene una Ãºnica responsabilidad bien definida
2. **Inmutabilidad:** Los datos en la zona Archive nunca se modifican
3. **Idempotencia:** Ejecutar el pipeline mÃºltiples veces produce el mismo resultado
4. **AuditorÃ­a Completa:** Trazabilidad de cada registro desde su origen

### Diagrama de Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPA DE PRESENTACIÃ“N                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         streamlit_app_arvo.py (Interfaz Web)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAPA DE MODELADO ML                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   code_ml.py    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  ml_model/                   â”‚   â”‚
â”‚  â”‚  (Entrenamiento)â”‚         â”‚  â”œâ”€â”€ model_xgb.pkl           â”‚   â”‚
â”‚  â”‚                 â”‚         â”‚  â””â”€â”€ transformador.pkl       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CAPA DE INGENIERÃA DE DATOS (ETL)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              arvo_parket.py (Pipeline ETL)               â”‚   â”‚
â”‚  â”‚  Extract â†’ Transform â†’ Load â†’ Archive                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CAPA DE ALMACENAMIENTO                         â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  BRONZE (Landing)   â”‚  â”‚  ARCHIVE (HistÃ³rico)             â”‚ â”‚
â”‚  â”‚  registros_         â”‚  â”‚  registros_procesados/           â”‚ â”‚
â”‚  â”‚  pacientes/         â”‚  â”‚  â”œâ”€â”€ record_..._Manuel.avro      â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€ nuevo1.avro    â”‚  â”‚  â”œâ”€â”€ record_..._Ricardo.avro    â”‚ â”‚
â”‚  â”‚  â””â”€â”€ nuevo2.avro    â”‚  â”‚  â””â”€â”€ record_..._Ita.avro        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  GOLD (Data Lake Optimizado)                             â”‚  â”‚
â”‚  â”‚  datalake_maestro.parquet                                â”‚  â”‚
â”‚  â”‚  (Formato Columnar - Consultas RÃ¡pidas)                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Estructura de Directorios

```
arvo_parquet/
â”‚
â”œâ”€â”€ ğŸ–¥ï¸ CAPA DE PRESENTACIÃ“N
â”‚   â””â”€â”€ streamlit_app_arvo.py          # Interfaz web para captura de datos
â”‚
â”œâ”€â”€ ğŸ§  CAPA DE MODELADO ML
â”‚   â”œâ”€â”€ code_ml.py                     # Entrenamiento y lÃ³gica del modelo
â”‚   â”œâ”€â”€ heart_disease_uci.csv          # Dataset original (UCI Heart Disease)
â”‚   â”‚
â”‚   â””â”€â”€ ml_model/                      # ğŸ’¾ Artefactos del modelo
â”‚       â”œâ”€â”€ model_xgb.pkl              # Modelo XGBoost entrenado (2.3 MB)
â”‚       â””â”€â”€ transformador.pkl          # Pipeline de preprocesamiento (45 KB)
â”‚
â”œâ”€â”€ âš™ï¸ CAPA DE INGENIERÃA DE DATOS
â”‚   â””â”€â”€ arvo_parket.py                 # Script ETL (Avro â†’ Parquet)
â”‚
â”œâ”€â”€ ğŸ“¥ ZONA BRONZE (Landing Zone)
â”‚   â””â”€â”€ registros_pacientes/           # Archivos .avro en crudo
â”‚       â””â”€â”€ [Archivos pendientes de procesar]
â”‚
â”œâ”€â”€ ğŸ“¦ ZONA ARCHIVE (HistÃ³rico)
â”‚   â””â”€â”€ registros_procesados/          # .avro procesados (auditorÃ­a)
â”‚       â”œâ”€â”€ record_20260101_212921_Manuel.avro
â”‚       â”œâ”€â”€ record_20260101_213010_Ricardo.avro
â”‚       â””â”€â”€ record_20260101_213032_Ita.avro
â”‚
â””â”€â”€ ğŸ† ZONA GOLD (Data Lake Optimizado)
    â””â”€â”€ datalake_maestro.parquet       # Base de datos columnar unificada
```

### Detalles de Archivos Clave

| Archivo | TamaÃ±o Aprox. | PropÃ³sito |
|---------|---------------|-----------|
| `streamlit_app_arvo.py` | ~15 KB | Interfaz de usuario y lÃ³gica de predicciÃ³n |
| `code_ml.py` | ~8 KB | Entrenamiento del modelo XGBoost |
| `arvo_parket.py` | ~5 KB | Pipeline ETL automatizable |
| `model_xgb.pkl` | ~2.3 MB | Modelo entrenado serializado |
| `transformador.pkl` | ~45 KB | Preprocesador (scaler + encoder) |
| `heart_disease_uci.csv` | ~8 KB | Dataset de 303 pacientes |
| `datalake_maestro.parquet` | Variable | Base de datos columnar (crece con el tiempo) |

---

## ğŸ”„ Flujo de Datos (Data Pipeline)

### DescripciÃ³n del Flujo Paso a Paso

#### Fase 1: Captura y PredicciÃ³n (Tiempo Real)
1. **Usuario ingresa datos** en formulario Streamlit
2. **Modelo ML procesa** los datos usando transformador + XGBoost
3. **Resultado mostrado** en pantalla (probabilidad de riesgo)
4. **Persistencia atÃ³mica** del registro en formato Avro

#### Fase 2: ConsolidaciÃ³n (Batch)
5. **Trigger manual/automÃ¡tico** ejecuta `arvo_parket.py`
6. **ExtracciÃ³n** de todos los `.avro` pendientes
7. **TransformaciÃ³n** a DataFrame unificado con validaciones
8. **Carga** incremental a Parquet (append o merge)
9. **Archivo** de registros procesados para auditorÃ­a

#### Fase 3: Consumo (AnÃ¡lisis)
10. **Herramientas BI** conectan directamente al Parquet
11. **Scripts Python** realizan anÃ¡lisis estadÃ­sticos

---

## ğŸ§© DescripciÃ³n de Componentes

### 1ï¸âƒ£ **streamlit_app_arvo.py** - Interfaz de Usuario

**Responsabilidades:**
- âœ… Capturar datos demogrÃ¡ficos y clÃ­nicos del paciente
- âœ… Validar entrada del usuario (rangos, tipos de datos)
- âœ… Invocar modelo ML para predicciÃ³n en tiempo real
- âœ… Persistir cada registro como archivo Avro individual
- âœ… Asignar nombres de archivo con timestamp Ãºnico

**TecnologÃ­as:**
- `streamlit` - Framework web interactivo
- `fastavro` - SerializaciÃ³n Avro
- `joblib` - Carga de modelos pickle
- `pandas` - ManipulaciÃ³n de datos

**Ejemplo de CÃ³digo (Simplificado):**

```python
import streamlit as st
import joblib
import fastavro
from datetime import datetime

# Cargar modelo
modelo = joblib.load('ml_model/model_xgb.pkl')
transformador = joblib.load('ml_model/transformador.pkl')

# Formulario
st.title("ğŸ¥ PredicciÃ³n de Riesgo CardÃ­aco")
nombre = st.text_input("Nombre del paciente")
edad = st.number_input("Edad", min_value=18, max_value=120)
sexo = st.selectbox("Sexo", ["Masculino", "Femenino"])
presion = st.number_input("PresiÃ³n arterial sistÃ³lica (mmHg)")

if st.button("Predecir Riesgo"):
    # Preparar datos
    datos = {
        'nombre': nombre,
        'edad': edad,
        'sexo': sexo,
        'presion_arterial': presion,
    }
    
    # PredicciÃ³n
    X = transformador.transform([datos])
    probabilidad = modelo.predict_proba(X)[0][1]
    
    st.success(f"Probabilidad de riesgo: {probabilidad:.2%}")
    
    # Guardar en Avro
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"registros_pacientes/record_{timestamp}_{nombre}.avro"
    
    esquema = {
        "type": "record",
        "name": "RegistroPaciente",
        "fields": [
            {"name": "nombre", "type": "string"},
            {"name": "edad", "type": "int"},
            {"name": "probabilidad_riesgo", "type": "double"},
        ]
    }
    
    with open(filename, 'wb') as f:
        fastavro.writer(f, esquema, [datos])
    
    st.info(f"âœ… Registro guardado: {filename}")
```

**CaracterÃ­sticas Clave:**
- **PredicciÃ³n InstantÃ¡nea:** Sin latencia perceptible (< 100ms)
- **ValidaciÃ³n en Tiempo Real:** Feedback inmediato al usuario
- **Persistencia AtÃ³mica:** Cada escritura es una transacciÃ³n completa
- **Naming Convention:** `record_YYYYMMDD_HHMMSS_Nombre.avro`

---

### 2ï¸âƒ£ **code_ml.py** - Motor de Machine Learning

**Responsabilidades:**
- âœ… Cargar y limpiar dataset UCI Heart Disease
- âœ… IngenierÃ­a de caracterÃ­sticas (feature engineering)
- âœ… Entrenar modelo XGBoost con validaciÃ³n cruzada
- âœ… Optimizar hiperparÃ¡metros (opcional: GridSearch)
- âœ… Serializar modelo y transformador

**Algoritmo:** XGBoost Classifier (Gradient Boosting)

**Pipeline de Preprocesamiento:**

```python
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline

# Definir columnas numÃ©ricas y categÃ³ricas
cols_numericas = ['edad', 'presion_arterial', 'colesterol', 'freq_cardiaca_max']
cols_categoricas = ['sexo', 'tipo_dolor_pecho', 'azucar_sangre', 'ecg_reposo']

# Crear transformador
transformador = ColumnTransformer([
    ('scaler_num', StandardScaler(), cols_numericas),
    ('encoder_cat', OneHotEncoder(drop='first', sparse_output=False), cols_categoricas)
])

# Pipeline completo
pipeline = Pipeline([
    ('transformador', transformador),
    ('modelo', XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    ))
])
```

**Flujo de Entrenamiento:**

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
import joblib

# 1. Cargar datos
df = pd.read_csv('heart_disease_uci.csv')

# 2. Limpieza
df = df.dropna()

# 3. Split train/test
X = df.drop('target', axis=1)
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 4. Entrenar
pipeline.fit(X_train, y_train)

# 5. Evaluar
from sklearn.metrics import roc_auc_score
y_pred_proba = pipeline.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_pred_proba)
print(f"AUC-ROC: {auc:.3f}")

# 6. Guardar
joblib.dump(pipeline.named_steps['modelo'], 'ml_model/model_xgb.pkl')
joblib.dump(pipeline.named_steps['transformador'], 'ml_model/transformador.pkl')
```

**MÃ©tricas TÃ­picas:**
- **AUC-ROC:** 0.85 - 0.90 (excelente discriminaciÃ³n)
- **Accuracy:** ~85%
- **Sensibilidad (Recall):** ~82% (detecta 82% de casos positivos)
- **Especificidad:** ~87% (descarta 87% de casos negativos)

---

### 3ï¸âƒ£ **arvo_parket.py** - Pipeline ETL

**Responsabilidades:**
- âœ… **Extract:** Leer todos los `.avro` de `registros_pacientes/`
- âœ… **Transform:** Unificar en DataFrame Pandas con validaciÃ³n de esquema
- âœ… **Load:** Escribir/anexar datos a `datalake_maestro.parquet`
- âœ… **Archive:** Mover archivos procesados a zona histÃ³rica
- âœ… **Logging:** Registrar operaciones y errores

**CÃ³digo Completo:**

```python
import os
import glob
import shutil
import pandas as pd
import fastavro
import pyarrow.parquet as pq
from datetime import datetime

def procesar_pipeline():
    """
    Pipeline ETL completo para consolidar archivos Avro a Parquet
    """
    
    # ===== EXTRACT =====
    print("ğŸ” Fase 1: Extrayendo archivos Avro...")
    archivos_pendientes = glob.glob('registros_pacientes/*.avro')
    
    if not archivos_pendientes:
        print("âš ï¸  No hay archivos pendientes de procesar")
        return
    
    print(f"   Encontrados: {len(archivos_pendientes)} archivos")
    
    # ===== TRANSFORM =====
    print("\nğŸ”„ Fase 2: Transformando datos...")
    registros = []
    
    for archivo in archivos_pendientes:
        try:
            with open(archivo, 'rb') as f:
                avro_reader = fastavro.reader(f)
                for registro in avro_reader:
                    # Agregar metadatos de auditorÃ­a
                    registro['_ingested_at'] = datetime.now().isoformat()
                    registro['_source_file'] = os.path.basename(archivo)
                    registros.append(registro)
        except Exception as e:
            print(f"   âŒ Error leyendo {archivo}: {e}")
            continue
    
    if not registros:
        print("   âš ï¸  No se pudieron extraer registros vÃ¡lidos")
        return
    
    df_nuevo = pd.DataFrame(registros)
    print(f"   âœ… Unificados {len(df_nuevo)} registros")
    
    # ValidaciÃ³n de esquema
    columnas_requeridas = ['nombre', 'edad', 'probabilidad_riesgo']
    if not all(col in df_nuevo.columns for col in columnas_requeridas):
        raise ValueError("Esquema invÃ¡lido: faltan columnas requeridas")
    
    # ===== LOAD =====
    print("\nğŸ’¾ Fase 3: Cargando a Parquet...")
    parquet_path = 'datalake_maestro.parquet'
    
    if os.path.exists(parquet_path):
        # Append incremental
        df_existente = pd.read_parquet(parquet_path)
        df_consolidado = pd.concat([df_existente, df_nuevo], ignore_index=True)
        
        # DeduplicaciÃ³n
        df_consolidado = df_consolidado.drop_duplicates(
            subset=['nombre', '_ingested_at'], 
            keep='last'
        )
    else:
        # Primera escritura
        df_consolidado = df_nuevo
    
    # Escribir con compresiÃ³n
    df_consolidado.to_parquet(
        parquet_path,
        engine='pyarrow',
        compression='snappy',
        index=False
    )
    
    print(f"   âœ… Escritos {len(df_consolidado)} registros totales")
    print(f"   ğŸ“¦ TamaÃ±o: {os.path.getsize(parquet_path) / 1024:.2f} KB")
    
    # ===== ARCHIVE =====
    print("\nğŸ“¦ Fase 4: Archivando procesados...")
    os.makedirs('registros_procesados', exist_ok=True)
    
    for archivo in archivos_pendientes:
        destino = os.path.join('registros_procesados', os.path.basename(archivo))
        shutil.move(archivo, destino)
        print(f"   âœ… Movido: {os.path.basename(archivo)}")
    
    print("\nğŸ‰ Pipeline completado exitosamente")

if __name__ == "__main__":
    procesar_pipeline()
```

---

## ğŸ—„ï¸ Zonas de Datos

### Arquitectura Medallion Simplificada

| Zona | Directorio | Formato | PropÃ³sito | CaracterÃ­sticas |
|------|-----------|---------|-----------|-----------------|
| **Bronze** | `registros_pacientes/` | Avro | Ingesta transaccional rÃ¡pida | Orientado a fila, esquema embebido |
| **Archive** | `registros_procesados/` | Avro | AuditorÃ­a e historial inmutable | Backup completo, no se modifica |
| **Gold** | `datalake_maestro.parquet` | Parquet | Consultas analÃ­ticas optimizadas | Orientado a columna, compresiÃ³n inteligente |

### Â¿Por quÃ© Avro en Bronze?

**Ventajas:**
- âœ… **Orientado a fila:** Ideal para escrituras de un registro a la vez
- âœ… **Esquema embebido:** AutovalidaciÃ³n de estructura
- âœ… **Compacto:** SerializaciÃ³n binaria eficiente (50% mÃ¡s pequeÃ±o que JSON)
- âœ… **EvoluciÃ³n de esquema:** Compatible con cambios hacia adelante/atrÃ¡s

**Casos de uso ideales:**
- Logs de eventos en tiempo real
- Sistemas transaccionales (OLTP)
- Streaming de datos (Kafka)

### Â¿Por quÃ© Parquet en Gold?

**Ventajas:**
- âœ… **Orientado a columna:** Consultas 100x mÃ¡s rÃ¡pidas para agregaciones
- âœ… **CompresiÃ³n inteligente:** 5-10x menos espacio que CSV
- âœ… **Predicate pushdown:** Lee solo las columnas y filas necesarias
- âœ… **Compatible con BI:** Power BI, Tableau, Apache Spark, DuckDB

**Casos de uso ideales:**
- AnÃ¡lisis de datos (OLAP)
- Data warehouses
- Machine Learning feature stores

---

## ğŸš€ GuÃ­a de InstalaciÃ³n

### Prerequisitos

- **Python:** 3.9 o superior
- **Sistema Operativo:** Linux, macOS, o Windows
- **RAM:** MÃ­nimo 2 GB (recomendado 4 GB)
- **Disco:** 500 MB libres

### Paso 1: Crear Entorno Virtual

```bash
# Linux/Mac
python3 -m venv webdjango
source webdjango/bin/activate

# Windows
python -m venv webdjango
webdjango\Scripts\activate
```

### Paso 2: Instalar Dependencias

```bash
# Actualizar pip
pip install --upgrade pip

# Instalar paquetes core
pip install streamlit pandas numpy scikit-learn xgboost joblib fastavro pyarrow

# Verificar instalaciÃ³n
python -c "import streamlit; print(f'Streamlit {streamlit.__version__}')"
python -c "import xgboost; print(f'XGBoost {xgboost.__version__}')"
```

### Paso 3: Crear Estructura de Directorios

```bash
mkdir -p registros_pacientes registros_procesados ml_model
```

### VerificaciÃ³n de InstalaciÃ³n

```bash
# Debe mostrar las versiones instaladas
pip list | grep -E "streamlit|xgboost|fastavro|pyarrow"
```

**Output esperado:**
```
fastavro           1.9.0
pyarrow            14.0.1
streamlit          1.28.2
xgboost            2.0.3
```

---

## ğŸ“– GuÃ­a de Uso

### Flujo Completo de Trabajo

#### Paso 1: Entrenar el Modelo (Una sola vez)

```bash
python code_ml.py
```

**Output esperado:**

```
ğŸ§  Iniciando entrenamiento del modelo...

ğŸ“Š Cargando dataset UCI Heart Disease...
   âœ… Cargados 303 registros

ğŸ”„ Preprocesando datos...
   âœ… ImputaciÃ³n de valores faltantes completada
   âœ… Split train/test: 242 / 61 registros

ğŸš€ Entrenando XGBoost Classifier...
   [0]     validation_0-logloss:0.62341
   [10]    validation_0-logloss:0.45123
   [20]    validation_0-logloss:0.38456
   [50]    validation_0-logloss:0.32109
   [99]    validation_0-logloss:0.29871

âœ… Entrenamiento completado

ğŸ“Š MÃ©tricas en conjunto de prueba:
   - AUC-ROC: 0.873
   - Accuracy: 0.852
   - Sensibilidad: 0.821
   - Especificidad: 0.876

ğŸ’¾ Guardando artefactos...
   âœ… ml_model/model_xgb.pkl (2.3 MB)
   âœ… ml_model/transformador.pkl (45 KB)

ğŸ‰ Modelo entrenado exitosamente
```

---

#### Paso 2: Capturar Datos de Pacientes

```bash
streamlit run streamlit_app_arvo.py
```

**Acciones en la interfaz:**

1. **Abrir navegador** en `http://localhost:8501`
2. **Completar formulario mÃ©dico:**
   - Nombre del paciente
   - Edad (18-120 aÃ±os)
   - Sexo (Masculino/Femenino)
   - PresiÃ³n arterial sistÃ³lica (mmHg)
   - Colesterol sÃ©rico (mg/dl)
   - Frecuencia cardÃ­aca mÃ¡xima
   - Y mÃ¡s campos clÃ­nicos...

3. **Click en "Predecir Riesgo"**
4. **Ver resultado:**
   ```
   ğŸ¯ Probabilidad de Riesgo CardÃ­aco: 68.3%
   âš ï¸  ClasificaciÃ³n: Riesgo Alto
   ```

5. **ConfirmaciÃ³n de guardado:**
   ```
   âœ… Registro guardado exitosamente
   ğŸ“„ Archivo: registros_pacientes/record_20260101_152030_JuanPerez.avro
   ```

**Resultado:** Se crea automÃ¡ticamente un archivo `.avro` en la carpeta `registros_pacientes/`

---

#### Paso 3: Consolidar a Parquet (ETL)

```bash
python arvo_parket.py
```

**Output en consola:**

```
ğŸ” Fase 1: Extrayendo archivos Avro...
   Encontrados: 5 archivos

ğŸ”„ Fase 2: Transformando datos...
   âœ… Unificados 5 registros
   âœ… ValidaciÃ³n de esquema: OK

ğŸ’¾ Fase 3: Cargando a Parquet...
   â„¹ï¸  Detectado archivo existente, anexando...
   âœ… Escritos 23 registros totales
   ï¿½ï¿½ TamaÃ±o del archivo: 3.47 KB
   ğŸ“ˆ Tasa de compresiÃ³n: 8.2x vs CSV

ğŸ“¦ Fase 4: Archivando procesados...
   âœ… Movido: record_20260101_212921_Manuel.avro
   âœ… Movido: record_20260101_213010_Ricardo.avro
   âœ… Movido: record_20260101_213032_Ita.avro
   âœ… Movido: record_20260101_214505_Ana.avro
   âœ… Movido: record_20260101_215120_Carlos.avro

ğŸ‰ Pipeline completado exitosamente
   â±ï¸  Tiempo de ejecuciÃ³n: 0.23 segundos
```

---

#### Paso 4: Consultar Data Lake (AnÃ¡lisis)

```python
import pandas as pd
import pyarrow.parquet as pq

# ===== CONSULTA BÃSICA =====
# Leer solo columnas necesarias (optimizaciÃ³n)
df = pd.read_parquet(
    'datalake_maestro.parquet',
    columns=['nombre', 'edad', 'probabilidad_riesgo']
)

print(df.head())
```

**Output:**
```
           nombre  edad  probabilidad_riesgo
0          Manuel    45                0.683
1         Ricardo    62                0.891
2             Ita    38                0.234
3             Ana    55                0.756
4          Carlos    71                0.923
```

```python
# ===== ANÃLISIS ESTADÃSTICO =====
print(f"Total de pacientes: {len(df)}")
print(f"Riesgo promedio: {df['probabilidad_riesgo'].mean():.2%}")
print(f"Edad promedio: {df['edad'].mean():.1f} aÃ±os")

# Pacientes de alto riesgo (>70%)
alto_riesgo = df[df['probabilidad_riesgo'] > 0.7]
print(f"Pacientes de alto riesgo: {len(alto_riesgo)} ({len(alto_riesgo)/len(df):.1%})")

# ===== CONSULTA CON FILTROS =====
# Gracias a Parquet, esto es ultra rÃ¡pido (predicate pushdown)
pacientes_mayores = pd.read_parquet(
    'datalake_maestro.parquet',
    filters=[('edad', '>', 60)]
)

# ===== EXPORTAR A OTROS FORMATOS =====
df.to_csv('reporte_pacientes.csv', index=False)
df.to_excel('reporte_pacientes.xlsx', index=False)

print("âœ… Reportes exportados")
```

---

## ğŸ“Š AnÃ¡lisis de Datos

### Ejemplo 1: Dashboard Simple con Pandas

```python
import pandas as pd
import matplotlib.pyplot as plt

# Cargar datos
df = pd.read_parquet('datalake_maestro.parquet')

# DistribuciÃ³n de riesgo por edad
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# GrÃ¡fico 1: Histograma de edad
axes[0].hist(df['edad'], bins=20, edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Edad')
axes[0].set_ylabel('Frecuencia')
axes[0].set_title('DistribuciÃ³n de Edad')

# GrÃ¡fico 2: Boxplot de riesgo por sexo
df.boxplot(column='probabilidad_riesgo', by='sexo', ax=axes[1])
axes[1].set_xlabel('Sex
